import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
from datetime import datetime
from BitNetMCU import FCMNIST, QuantizedModel
import time

# Export quantized model from saved checkpoint
# cpldcpu 2024-04-14
# Note: Hyperparameters are used to generated the filename

#----------------------------------------------
# Define training hyperparameters here11

hyperparameters = {
    "num_epochs": 60,
    "QuantType": '3bitsym', # 'Ternary', 'Binary', 'BinaryBalanced', '2bitsym', '4bitsym', '8bit', 'None", 'FP130' 
    "BPW": 3,  # Bits per weight 
    "NormType": 'RMS', # 'RMS', 'Lin', 'BatchNorm'
    "WScale": 'PerTensor', # 'PerTensor', 'PerOutput', 'PerOutputLog2'
    "batch_size": 128,
    "learning_rate": 1e-3,
    "lr_decay": 0.1, # these are not used with cosine scheduler
    "step_size": 20,
    "network_width1": 80, 
    "network_width2": 80, 
    "network_width3": 80,
    "Augmentation": True,
    "runname": ''
}

# runtag = 'a11_Opt12k_cos_s00n'
# runtag = 'a11_Opt12k_cos'
# runtag = 'a11_Opt12k_cos_s00b16'
# runtag = 'a11_Opt12k_cos_mixed'
# runtag = 'a21_Opt12k_cos_mixed'
# runtag = 'a21_cos_scale4'
runtag = 'a21_cos_scale15'
#---------------------------------------------
exportfolder = 'model_h'
#---------------------------------------------

def create_run_name(hyperparameters):
    runname = runtag + ('_Aug' if hyperparameters["Augmentation"] else '') + '_BitMnist_' + hyperparameters["WScale"] + "_" +hyperparameters["QuantType"] + "_" + hyperparameters["NormType"] + "_width" + str(hyperparameters["network_width1"]) + "_" + str(hyperparameters["network_width2"]) + "_" + str(hyperparameters["network_width3"]) + "_lr" + str(hyperparameters["learning_rate"]) + "_decay" + str(hyperparameters["lr_decay"]) + "_stepsize" + str(hyperparameters["step_size"]) + "_bs" + str(hyperparameters["batch_size"]) + "_epochs" + str(hyperparameters["num_epochs"])
    hyperparameters["runname"] = runname
    return runname

def export_to_hfile(quantized_model, filename, runname):
    """
    Exports the quantized model to an Ansi-C header file.

    Parameters:
    filename (str): The name of the header file to which the quantized model will be exported.

    Note:
    This method currently only supports binary quantization. 
    """

    if not quantized_model.quantized_model:
        raise ValueError("quantized_model is empty or None")

    with open(filename, 'w') as f:
        f.write(f'// Automatically generated header file\n')
        f.write(f'// Date: {datetime.now()}\n')
        f.write(f'// Quantized model exported from {runname}.pth\n')
        f.write('// Generated by exportquant.py\n\n')
        
        f.write('#include <stdint.h>\n\n')
        for layer_info in quantized_model.quantized_model:
            layer= f'L{layer_info["layer_order"]}'
            incoming_weights = layer_info['incoming_weights']
            outgoing_weights = layer_info['outgoing_weights']
            bpw = layer_info['bpw']
            weights = np.array(layer_info['quantized_weights'])
            quantization_type = layer_info['quantization_type']

            if (bpw*incoming_weights%32) != 0:
                raise ValueError(f"Size mismatch: Incoming weights must be packed to 32bit boundary. Incoming weights: {incoming_weights} Bit per weight: {bpw} Total bits: {bpw*incoming_weights}")

            print(f'Layer: {layer} Quantization type: <{quantization_type}>, Bits per weight: {bpw}, Num. incoming: {incoming_weights},  Num outgoing: {outgoing_weights}')
            if quantization_type == 'Binary':
                encoded_weights = np.where(weights == -1, 0, 1)
                quant = 1
            elif quantization_type == '2bitsym': # encoding -1.5 -> 11, -0.5 -> 10, 0.5 -> 00, 1.5 -> 01 (one complement with offset)
                encoded_weights = ((weights < 0).astype(int) << 1) | (np.floor(np.abs(weights))).astype(int)  
                quant = 2
            elif quantization_type == '3bitsym': 
                encoded_weights = ((weights < 0).astype(int) << 2) | (np.floor(np.abs(weights))).astype(int)  
                quant = 3
            elif quantization_type == '3bitlog': # FP1.2.0 encoding (sign * 2^exp)
                encoded_weights = ((weights < 0).astype(int) << 2) | (np.floor(np.log2(np.abs(weights)))).astype(int)  
                quant = 3 + 16 # offset 16 to encode FP 1.3.0
            elif quantization_type == '4bitsym': 
                encoded_weights = ((weights < 0).astype(int) << 3) | (np.floor(np.abs(weights))).astype(int)  
                quant = 4
            elif quantization_type == '4bitlog': # FP1.3.0 encoding (sign * 2^exp)
                encoded_weights = ((weights < 0).astype(int) << 3) | (np.floor(np.log2(np.abs(weights)))).astype(int)  
                quant = 4 + 16 # offset 16 to encode FP 1.3.0
            else:
                print(f'Quantization type not supported. Layer {layer} with quantization type {quantization_type} and {bpw} bits per weight. ')
                exit()

            # pack bits into 32 bit words
            weight_per_word = 32 // bpw 
            reshaped_array = encoded_weights.reshape(-1, weight_per_word)
            bit_positions = 32 - bpw - np.arange(weight_per_word) * bpw
            packed_weights = np.bitwise_or.reduce(reshaped_array << bit_positions, axis=1).view(np.uint32)

            # print(f'weights: {weights.shape} {weights.flatten()[0:16]}')
            # print(f'Encoded weights: {encoded_weights.shape} {encoded_weights.flatten()[0:16]}')
            # print(f'Packed weights: {packed_weights.shape} {", ".join(map(lambda x: hex(x), packed_weights.flatten()[0:4]))}')

            # Write layer order, shape, shiftright and weights to the file
            f.write(f'// Layer: {layer}\n')
            f.write(f'// Quantization type: {quantization_type}\n')
            f.write(f'uint32_t {layer}_bitperweight = {quant};\n')
            f.write(f'uint32_t {layer}_incoming_weights = {incoming_weights};\n')
            f.write(f'uint32_t {layer}_outgoing_weights = {outgoing_weights};\n')
            f.write(f'uint32_t {layer}_weights[] = {{{", ".join(map(lambda x: hex(x), packed_weights.flatten()))}}};\n//first channel is topmost bit\n\n')

def print_stats(quantized_model):
    for layer_info in quantized_model.quantized_model:
        weights = np.array(layer_info['quantized_weights'])
        print()
        print(f'Layer: {layer_info["layer_order"]}, Max: {np.max(weights)}, Min: {np.min(weights)}, Mean: {np.mean(weights)}, Std: {np.std(weights)}')

        values, counts = np.unique(weights, return_counts=True)
        probabilities = counts / np.sum(counts)

        print(f'Values: {values}')
        print(f'Percent: {(probabilities * 100)}')

        number_of_codes = 2**layer_info['bpw'] 
        entropy = -np.sum(probabilities * np.log2(probabilities))
        print(f'Entropy: {entropy:.2f} bits. Code capacity used: {entropy / np.log2(number_of_codes) * 100} %')
   
if __name__ == '__main__':

    # main
    runname= create_run_name(hyperparameters)
    print(runname)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load the MNIST dataset
    transform = transforms.Compose([
        transforms.Resize((16, 16)),  # Resize images to 16x16
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_data = datasets.MNIST(root='data', train=True, transform=transform, download=True)
    test_data = datasets.MNIST(root='data', train=False, transform=transform)
    # Create data loaders
    # test_loader = DataLoader(test_data, batch_size=hyperparameters["batch_size"], shuffle=False)
    test_loader = DataLoader(test_data, batch_size=10000, shuffle=False)

    # Initialize the network and optimizer
    model = FCMNIST(
        network_width1=hyperparameters["network_width1"], 
        network_width2=hyperparameters["network_width2"], 
        network_width3=hyperparameters["network_width3"], 
        QuantType=hyperparameters["QuantType"], 
        NormType=hyperparameters["NormType"],
        WScale=hyperparameters["WScale"]
    ).to(device)

    print('Loading model...')    
    try:
        model.load_state_dict(torch.load(f'modeldata/{runname}.pth'))
    except FileNotFoundError:
        print(f"The file 'modeldata/{runname}.pth' does not exist.")
        exit()

    print('Inference using the original model...')
    correct = 0
    total = 0
    test_loss = []
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)        
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    testaccuracy = correct / total * 100
    print(f'Accuracy/Test of trained model: {testaccuracy} %')

    print('Quantizing model...')
    # Quantize the model
    quantized_model = QuantizedModel(model)
    print(f'Total number of bits: {quantized_model.totalbits()} ({quantized_model.totalbits()/8/1024} kbytes)')

    print_stats(quantized_model)

    # Inference using the quantized model
    print ("inference of quantized model")

    total_correct_predictions = 0
    total_samples = 0
    start_time = time.time()

    # Iterate over the test data
    for input_data, labels in test_loader:
        # Reshape and convert to numpy
        input_data = input_data.view(input_data.size(0), -1).cpu().numpy()
        labels = labels.cpu().numpy()

        # Inference
        result = quantized_model.inference_quantized(input_data)

        # Get predictions
        predict = np.argmax(result, axis=1)

        # Calculate the fraction of correct predictions for this batch
        correct_predictions = (predict == labels).sum()

        # Update counters
        total_correct_predictions += correct_predictions  # Multiply by batch size
        total_samples += input_data.shape[0]

    end_time = time.time()

    # Calculate and print the overall fraction of correct predictions
    overall_correct_predictions = total_correct_predictions / total_samples

    print(f"Time taken for inference and prediction: {end_time - start_time} seconds")
    print('Accuracy/Test of quantized model:', overall_correct_predictions * 100, '%') 

    print("Exporting model to header file")
    # export the quantized model to a header file
    # export_to_hfile(quantized_model, f'{exportfolder}/{runname}.h')
    export_to_hfile(quantized_model, f'BitNetMCU_model.h',runname)